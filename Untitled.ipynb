{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1c85d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import random\n",
    "import warnings\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from typing import Sequence\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple, deque\n",
    "from plot_script import plot_result, draw_neural_net\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9987b348",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "MIN_REPLAY_SIZE = 5000\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 0.995\n",
    "TARGET_UPDATE_FREQ = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85b2dc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "obs = env.reset()\n",
    "episode_reward = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "647730cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('states', 'actions', 'rewards', 'dones', 'next_states'))\n",
    "class Replay_memory():\n",
    "\n",
    "    def __init__(self, env, fullsize, minsize, batchsize):\n",
    "        self.env = env\n",
    "        self.memory = deque(maxlen=fullsize)\n",
    "        self.rewards = deque(maxlen=50)\n",
    "        self.batchsize = batchsize\n",
    "        self.minsize = minsize\n",
    "\n",
    "    def append(self, transition):\n",
    "        self.memory.append(transition)\n",
    "    \n",
    "    def sample_batch(self):\n",
    "        # Filter out empty tuples and handle observations with additional information\n",
    "        non_empty_batch = [obs for obs in self.memory if isinstance(obs.states[0], np.ndarray)]\n",
    "\n",
    "        # Check if there are enough non-empty transitions\n",
    "        if len(non_empty_batch) < self.batchsize:\n",
    "            #print(\"Not enough non-empty transitions.\")\n",
    "            return None\n",
    "\n",
    "        # Sample from the filtered batch\n",
    "        batch = random.sample(non_empty_batch, self.batchsize)\n",
    "        batch = Transition(*zip(*batch))\n",
    "\n",
    "        # Add debug prints\n",
    "        #print(\"States in batch:\")\n",
    "        for obs in batch.states:\n",
    "            if isinstance(obs[0], np.ndarray):  # Check if states[0] is iterable\n",
    "                state_array = obs[0]\n",
    "                #print(f\"Observation shape: {state_array.shape}\")\n",
    "                #print(f\"Observation: {state_array}\")\n",
    "            elif isinstance(obs, tuple) and isinstance(obs[0], np.ndarray):\n",
    "                state_array = obs[0]\n",
    "                #print(f\"Observation shape: {state_array.shape}\")\n",
    "                #print(f\"Observation: {state_array}\")\n",
    "            else:\n",
    "                print(\"Unsupported observation format\")\n",
    "                continue\n",
    "\n",
    "        # Check if there are any valid observations\n",
    "        valid_obs = [obs for obs in batch.states if isinstance(obs[0], np.ndarray)]\n",
    "        if not valid_obs:\n",
    "            # Handle the case when there are no valid observations\n",
    "            #print(\"No valid observations in the batch.\")\n",
    "            return None\n",
    "\n",
    "        # Find the maximum shape for each dimension\n",
    "        max_shapes = [max(obs[0].shape[j] for obs in valid_obs) for j in range(len(valid_obs[0][0].shape))]\n",
    "\n",
    "        #print(\"Max Shapes:\", max_shapes)\n",
    "\n",
    "        # Pad or crop state observations to have the same shape\n",
    "        padded_states = np.array([\n",
    "            [np.pad(obs[j], (0, max_shapes[j] - obs[j].shape[0])) for j in range(min(len(obs), len(max_shapes)))]\n",
    "            for obs in valid_obs\n",
    "        ])\n",
    "\n",
    "\n",
    "\n",
    "        states = torch.from_numpy(padded_states.astype(np.float32)).to(device)\n",
    "        actions = torch.from_numpy(np.array(batch.actions, dtype=np.int64)).to(device).unsqueeze(1)\n",
    "\n",
    "\n",
    "\n",
    "        rewards = torch.from_numpy(np.array(batch.rewards, dtype=np.float32)).unsqueeze(1).to(device)\n",
    "        dones = torch.from_numpy(np.array(batch.dones, dtype=np.bool8)).unsqueeze(1).to(device)\n",
    "        next_states = torch.from_numpy(np.array(batch.next_states, dtype=np.float32)).to(device)\n",
    "\n",
    "        return states, actions, rewards, dones, next_states\n",
    "\n",
    "    def initialize(self):\n",
    "        obs = env.reset()\n",
    "        for _ in range(self.minsize):\n",
    "            action = self.env.action_space.sample()\n",
    "            #print(env.step(action))\n",
    "            new_obs, reward, done, info, _ = env.step(action)\n",
    "            transition = Transition(obs, action, reward, done, new_obs)\n",
    "            self.append(transition)\n",
    "            obs = new_obs\n",
    "            if done:\n",
    "                self.env.reset()\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26b124fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_memory = Replay_memory(env, BUFFER_SIZE, MIN_REPLAY_SIZE, BATCH_SIZE).initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff5d431d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, ninputs, noutputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.a1 = nn.Linear(ninputs, 64, device=device)\n",
    "        self.a2 = nn.Linear(64, noutputs, device=device)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        o = self.a1(X)\n",
    "        o = torch.tanh(o)\n",
    "        o = self.a2(o)\n",
    "        \n",
    "        return o\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        return self.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "629d5e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (a1): Linear(in_features=8, out_features=64, bias=True)\n",
       "  (a2): Linear(in_features=64, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn_policy = DQN(env.observation_space.shape[0], env.action_space.n)\n",
    "dqn_target = DQN(env.observation_space.shape[0], env.action_space.n)\n",
    "dqn_target.load_state_dict(dqn_policy.state_dict())\n",
    "dqn_target.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fe95829",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.SmoothL1Loss()\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(dqn_policy.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db0e32a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(epsilon, obs):\n",
    "    rnd_sample = random.random()\n",
    "    \n",
    "    if rnd_sample <= epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            # Extract the state array from the observation tuple\n",
    "            state_array = obs[0] if isinstance(obs, tuple) else obs\n",
    "            obs_tensor = torch.Tensor(state_array).to(device)\n",
    "            action = int(torch.argmax(dqn_policy(obs_tensor)))\n",
    "    \n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d2e1d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100 Avg Results: -205.1715481053213 Epsilon: 0.6088145090359074\n",
      "Episode: 200 Avg Results: -340.15041594550667 Epsilon: 0.36880183088056995\n",
      "Episode: 300 Avg Results: -399.1352068865279 Epsilon: 0.22340924607110255\n",
      "Episode: 400 Avg Results: -264.6417827248626 Epsilon: 0.1353347165085562\n",
      "Episode: 500 Avg Results: -391.3400722584682 Epsilon: 0.08198177029173696\n",
      "Episode: 600 Avg Results: -329.97895601668256 Epsilon: 0.05\n",
      "Episode: 700 Avg Results: -244.6880772658695 Epsilon: 0.05\n",
      "Episode: 800 Avg Results: -143.95188489160543 Epsilon: 0.05\n",
      "Episode: 900 Avg Results: -174.76430046386463 Epsilon: 0.05\n",
      "Episode: 1000 Avg Results: -199.1591453015795 Epsilon: 0.05\n",
      "Episode: 1100 Avg Results: -190.8595251073349 Epsilon: 0.05\n",
      "Episode: 1200 Avg Results: -241.2524842499609 Epsilon: 0.05\n",
      "Episode: 1300 Avg Results: -159.14867219848483 Epsilon: 0.05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 16\u001b[0m\n\u001b[0;32m      9\u001b[0m action \u001b[38;5;241m=\u001b[39m epsilon_greedy_policy(eps_threshold, obs)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Print the shape of the observation\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#print(f'Observation: {obs}')\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Get the new observation and reward.\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m new_obs, reward, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Append to Replay Memory\u001b[39;00m\n\u001b[0;32m     19\u001b[0m replay_memory\u001b[38;5;241m.\u001b[39mappend(Transition(obs, action, reward, done, new_obs))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gym\\wrappers\\time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \n\u001b[0;32m     42\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gym\\wrappers\\order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gym\\wrappers\\env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gym\\envs\\box2d\\lunar_lander.py:556\u001b[0m, in \u001b[0;36mLunarLander.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    545\u001b[0m     p\u001b[38;5;241m.\u001b[39mApplyLinearImpulse(\n\u001b[0;32m    546\u001b[0m         (ox \u001b[38;5;241m*\u001b[39m SIDE_ENGINE_POWER \u001b[38;5;241m*\u001b[39m s_power, oy \u001b[38;5;241m*\u001b[39m SIDE_ENGINE_POWER \u001b[38;5;241m*\u001b[39m s_power),\n\u001b[0;32m    547\u001b[0m         impulse_pos,\n\u001b[0;32m    548\u001b[0m         \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    549\u001b[0m     )\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlander\u001b[38;5;241m.\u001b[39mApplyLinearImpulse(\n\u001b[0;32m    551\u001b[0m         (\u001b[38;5;241m-\u001b[39mox \u001b[38;5;241m*\u001b[39m SIDE_ENGINE_POWER \u001b[38;5;241m*\u001b[39m s_power, \u001b[38;5;241m-\u001b[39moy \u001b[38;5;241m*\u001b[39m SIDE_ENGINE_POWER \u001b[38;5;241m*\u001b[39m s_power),\n\u001b[0;32m    552\u001b[0m         impulse_pos,\n\u001b[0;32m    553\u001b[0m         \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    554\u001b[0m     )\n\u001b[1;32m--> 556\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworld\u001b[38;5;241m.\u001b[39mStep(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m FPS, \u001b[38;5;241m6\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m30\u001b[39m)\n\u001b[0;32m    558\u001b[0m pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlander\u001b[38;5;241m.\u001b[39mposition\n\u001b[0;32m    559\u001b[0m vel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlander\u001b[38;5;241m.\u001b[39mlinearVelocity\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gym\\envs\\box2d\\lunar_lander.py:61\u001b[0m, in \u001b[0;36mContactDetector.BeginContact\u001b[1;34m(self, contact)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mBeginContact\u001b[39m(\u001b[38;5;28mself\u001b[39m, contact):\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m---> 61\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mlander \u001b[38;5;241m==\u001b[39m contact\u001b[38;5;241m.\u001b[39mfixtureA\u001b[38;5;241m.\u001b[39mbody\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mlander \u001b[38;5;241m==\u001b[39m contact\u001b[38;5;241m.\u001b[39mfixtureB\u001b[38;5;241m.\u001b[39mbody\n\u001b[0;32m     63\u001b[0m     ):\n\u001b[0;32m     64\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mgame_over \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\Box2D\\Box2D.py:429\u001b[0m, in \u001b[0;36mb2BodyCompare\u001b[1;34m(a, b)\u001b[0m\n\u001b[0;32m    427\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    428\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m __shapeeq(a, b)\n\u001b[1;32m--> 429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mb2BodyCompare\u001b[39m(a, b):\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, b2Body) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(b, b2Body):\n\u001b[0;32m    431\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "eps_threshold = EPS_START\n",
    "episode = 1\n",
    "history = []\n",
    "\n",
    "for step in itertools.count():\n",
    "    \n",
    "    # Get action using Epsilon-Greedy Policy\n",
    "    action = epsilon_greedy_policy(eps_threshold, obs)\n",
    "\n",
    "    # Print the shape of the observation\n",
    "    #print(f'Observation: {obs}')\n",
    "\n",
    "\n",
    "    # Get the new observation and reward.\n",
    "    new_obs, reward, done, _, _ = env.step(action)\n",
    "\n",
    "    # Append to Replay Memory\n",
    "    replay_memory.append(Transition(obs, action, reward, done, new_obs))\n",
    "    episode_reward += reward\n",
    "    obs = new_obs\n",
    "    \n",
    "    # If the episode is finished\n",
    "    if done:\n",
    "        \n",
    "        episode += 1\n",
    "        eps_threshold = np.max((eps_threshold * EPS_DECAY, EPS_END))\n",
    "        replay_memory.rewards.append(episode_reward)\n",
    "        obs = env.reset()\n",
    "        avg_res = np.mean(replay_memory.rewards)\n",
    "        history.append((episode-1, avg_res))\n",
    "        \n",
    "        if episode % 100 == 0: \n",
    "            print(f'Episode: {episode} Avg Results: {avg_res} Epsilon: {eps_threshold}')\n",
    "\n",
    "        if avg_res >= 195:\n",
    "            print(f'Solved at episode: {episode} Avg Results: {avg_res}')\n",
    "            break\n",
    "        \n",
    "        if step % TARGET_UPDATE_FREQ == 0:\n",
    "            dqn_target.load_state_dict(dqn_policy.state_dict())\n",
    "\n",
    "        episode_reward = 0\n",
    "    \n",
    "    # Sample from the Replay Memory\n",
    "    batch_result = replay_memory.sample_batch()\n",
    "\n",
    "    # Check if sample_batch returned None\n",
    "    if batch_result is None:\n",
    "        continue  # Skip this iteration if there are not enough non-empty transitions\n",
    "\n",
    "    b_states, b_actions, b_rewards, b_dones, b_next_states = batch_result\n",
    "    \n",
    "    # Get Q-Values of every state-action pair from the Replay Memory Sample\n",
    "    #print(\"Shapes before gather:\", b_states.shape, b_actions.shape)\n",
    "    qvalues = dqn_policy(b_states).gather(2, b_actions.unsqueeze(2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    # Train the Neural Network to better evaluate the states observed.\n",
    "    with torch.no_grad():\n",
    "        target_qvalues = dqn_target(b_next_states)\n",
    "        max_target_qvalues = torch.max(target_qvalues, axis=1).values.unsqueeze(1)\n",
    "        expected_qvalues = b_rewards + GAMMA * (1 - b_dones.type(torch.int64)) * max_target_qvalues\n",
    "\n",
    "    loss = loss_fn(qvalues, expected_qvalues)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in dqn_policy.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99489ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = pd.DataFrame(\n",
    "    history, columns=['Episode', 'Score'])\n",
    "\n",
    "(fig, ax) = plt.subplots(1, 1)\n",
    "\n",
    "ax.plot(history['Episode'], history['Score'])\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Score over last 50 episodes')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
